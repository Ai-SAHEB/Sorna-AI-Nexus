# mr_majid@riseup.net
import os
import sys
import json
import time
import requests
import numpy as np
import random
import re
import hashlib
import secrets
import string
import sqlite3
import psutil
import urllib.parse
import zipfile
import base64
import pickle
from collections import defaultdict, deque
from datetime import datetime, timedelta
import threading
import queue
import getpass
import logging
from typing import Dict, List, Any, Optional

print("=" * 70)
print("ğŸ§  SORNA AI NEXUS - ULTIMATE AUTONOMOUS SELF-EVOLVING SYSTEM")
print("ğŸš€ GitHub Actions Optimized - Full Autonomy Edition")
print("ğŸ¯ Connected to: https://github.com/Ai-SAHEB/Sorna-AI-Nexus")
print("=" * 70)

# ==================== Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØªÙˆÚ©Ù† Ø§Ù…Ù† ====================
class SecureTokenManager:
    def __init__(self):
        self.token_file = "github_token.enc"
        self.encryption_key = self._generate_encryption_key()
    
    def _generate_encryption_key(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ Ø§Ù…Ù†"""
        return hashlib.sha256(b"SornaAISecretKey2024").digest()
    
    def save_token(self, token: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ø§Ù…Ù† ØªÙˆÚ©Ù†"""
        try:
            encrypted = base64.b64encode(token.encode()).decode()
            with open(self.token_file, 'w') as f:
                json.dump({'token': encrypted}, f)
            return True
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªÙˆÚ©Ù†: {e}")
            return False
    
    def load_token(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†"""
        try:
            if os.path.exists(self.token_file):
                with open(self.token_file, 'r') as f:
                    data = json.load(f)
                    return base64.b64decode(data['token']).decode()
            return None
        except Exception:
            return None

# ==================== ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ ====================
class RealGitHubIntegration:
    def __init__(self, token_manager):
        self.token_manager = token_manager
        self.token = os.getenv('GITHUB_TOKEN', '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')
        self.connected = False
        self.headers = {}
        self.repo_owner = "Ai-SAHEB"
        self.repo_name = "Sorna-AI-Nexus"
        self.base_url = "https://api.github.com"
        self.logger = AdvancedLogger()
    
    def connect(self):
        """Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨"""
        try:
            self.headers = {
                'Authorization': f'token {self.token}',
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'Sorna-AI-Nexus'
            }
            
            # ØªØ³Øª Ø§ØªØµØ§Ù„
            response = requests.get(
                f"{self.base_url}/user",
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                self.connected = True
                user_data = response.json()
                self.logger.info(f"âœ… Ù…ØªØµÙ„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù†: {user_data.get('login', 'Unknown')}")
                return True
            else:
                self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {e}")
            return False
    
    def create_file_in_repo(self, file_path, content, commit_message):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø¯Ø± Ø±ÛŒÙ¾ÙˆÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨"""
        if not self.connected:
            self.logger.warning("Ø§ØªØµØ§Ù„ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³Øª")
            return False
        
        try:
            url = f"{self.base_url}/repos/{self.repo_owner}/{self.repo_name}/contents/{file_path}"
            
            data = {
                "message": commit_message,
                "content": base64.b64encode(content.encode()).decode(),
                "branch": "main"
            }
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
            check_response = requests.get(url, headers=self.headers)
            if check_response.status_code == 200:
                existing_data = check_response.json()
                data["sha"] = existing_data["sha"]
            
            response = requests.put(url, headers=self.headers, json=data, timeout=30)
            
            if response.status_code in [200, 201]:
                self.logger.info(f"âœ… ÙØ§ÛŒÙ„ {file_path} Ø¯Ø± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯")
                return True
            else:
                self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {e}")
            return False
    
    def get_repo_contents(self, path=""):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ Ø±ÛŒÙ¾Ùˆ"""
        try:
            url = f"{self.base_url}/repos/{self.repo_owner}/{self.repo_name}/contents/{path}"
            response = requests.get(url, headers=self.headers)
            return response.json() if response.status_code == 200 else []
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ Ø±ÛŒÙ¾Ùˆ: {e}")
            return []

# ==================== Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class AdvancedLogger:
    def __init__(self):
        self.logger = logging.getLogger('SornaAI')
        self.logger.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        file_handler = logging.FileHandler('sorna_evolution.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def info(self, message):
        self.logger.info(message)
    
    def warning(self, message):
        self.logger.warning(message)
    
    def error(self, message):
        self.logger.error(message)
    
    def evolution(self, message):
        self.logger.info(f"ğŸ¯ EVOLUTION: {message}")

# ==================== Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class AdvancedMemorySystem:
    def __init__(self):
        self.db_path = "sorna_memory.db"
        self.logger = AdvancedLogger()
        self.init_database()
    
    def init_database(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conceptual_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                concept TEXT UNIQUE,
                description TEXT,
                category TEXT,
                confidence REAL DEFAULT 0.8,
                source TEXT DEFAULT 'auto_learned',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                access_count INTEGER DEFAULT 1,
                importance_score REAL DEFAULT 0.5
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_experiences (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                experience_type TEXT,
                input_data TEXT,
                output_data TEXT,
                success_rate REAL,
                lesson_learned TEXT,
                context TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS success_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT,
                pattern_data TEXT,
                success_count INTEGER DEFAULT 1,
                failure_count INTEGER DEFAULT 0,
                last_used TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                effectiveness REAL DEFAULT 0.8
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS system_state (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        self.logger.info("Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def save_knowledge(self, concept: str, description: str, category: str, confidence: float = 0.8):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´ Ø¬Ø¯ÛŒØ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO conceptual_knowledge 
                (concept, description, category, confidence, last_accessed, access_count)
                VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, 
                COALESCE((SELECT access_count FROM conceptual_knowledge WHERE concept = ?), 0) + 1)
            ''', (concept, description, category, confidence, concept))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´: {e}")
            return False
    
    def get_knowledge(self, concept: str):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ù†Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙÙ‡ÙˆÙ…"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT concept, description, category, confidence, access_count 
                FROM conceptual_knowledge WHERE concept = ?
            ''', (concept,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return {
                    'concept': result[0],
                    'description': result[1],
                    'category': result[2],
                    'confidence': result[3],
                    'access_count': result[4]
                }
            return None
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ù†Ø´: {e}")
            return None
    
    def record_experience(self, exp_type: str, input_data: str, output_data: str, 
                         success: bool, lesson: str, context: str = ""):
        """Ø«Ø¨Øª ØªØ¬Ø±Ø¨Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        try:
            success_rate = 1.0 if success else 0.0
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO learning_experiences 
                (experience_type, input_data, output_data, success_rate, lesson_learned, context)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (exp_type, input_data, output_data, success_rate, lesson, context))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª ØªØ¬Ø±Ø¨Ù‡: {e}")
            return False

# ==================== Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø± ====================
class PersistentMemorySystem:
    def __init__(self):
        self.memory_dir = "memory"
        self.knowledge_file = f"{self.memory_dir}/knowledge_base.json"
        self.learning_file = f"{self.memory_dir}/learning_progress.json"
        self.conversation_file = f"{self.memory_dir}/conversation_history.json"
        self.research_file = f"{self.memory_dir}/research_topics.json"
        self.logger = AdvancedLogger()
        self.setup_memory_system()
    
    def setup_memory_system(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±"""
        os.makedirs(self.memory_dir, exist_ok=True)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯
        initial_data = {
            'knowledge_base.json': {'concepts': {}, 'categories': {}, 'created_at': datetime.now().isoformat()},
            'learning_progress.json': {'daily_progress': {}, 'milestones': [], 'learning_goals': {}},
            'conversation_history.json': {'conversations': [], 'user_profiles': {}},
            'research_topics.json': {'topics': {}, 'research_history': [], 'discoveries': []}
        }
        
        for file_path, data in initial_data.items():
            full_path = f"{self.memory_dir}/{file_path}"
            if not os.path.exists(full_path):
                with open(full_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info("âœ… Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def save_conversation(self, user_input: str, ai_response: str, context: dict = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡"""
        try:
            with open(self.conversation_file, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                
                conversation = {
                    'timestamp': datetime.now().isoformat(),
                    'user_input': user_input,
                    'ai_response': ai_response,
                    'context': context or {},
                    'topics': self.extract_topics(user_input),
                    'sentiment': self.analyze_sentiment(user_input)
                }
                
                data['conversations'].append(conversation)
                
                # Ø­ÙØ¸ ÙÙ‚Ø· 100Û° Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¢Ø®Ø±
                if len(data['conversations']) > 1000:
                    data['conversations'] = data['conversations'][-500:]
                
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()
            
            return True
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡: {e}")
            return False
    
    def get_conversation_history(self, limit: int = 50):
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª"""
        try:
            with open(self.conversation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data['conversations'][-limit:]
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡: {e}")
            return []
    
    def update_learning_progress(self, topic: str, progress: float, notes: str = ""):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        try:
            with open(self.learning_file, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                
                today = datetime.now().strftime('%Y-%m-%d')
                if today not in data['daily_progress']:
                    data['daily_progress'][today] = {}
                
                data['daily_progress'][today][topic] = {
                    'progress': progress,
                    'notes': notes,
                    'updated_at': datetime.now().isoformat()
                }
                
                # Ø¨Ø±Ø±Ø³ÛŒ milestones
                if progress >= 0.8 and topic not in [m['topic'] for m in data['milestones']]:
                    data['milestones'].append({
                        'topic': topic,
                        'achieved_at': datetime.now().isoformat(),
                        'progress': progress
                    })
                
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()
            
            return True
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØª: {e}")
            return False
    
    def save_research_topic(self, topic: str, findings: dict, sources: list = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆØ¶ÙˆØ¹ ØªØ­Ù‚ÛŒÙ‚ÛŒ Ùˆ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§"""
        try:
            with open(self.research_file, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                
                research_entry = {
                    'topic': topic,
                    'findings': findings,
                    'sources': sources or [],
                    'researched_at': datetime.now().isoformat(),
                    'confidence': findings.get('confidence', 0.5)
                }
                
                data['research_history'].append(research_entry)
                
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ topics
                if topic not in data['topics']:
                    data['topics'][topic] = {
                        'first_researched': datetime.now().isoformat(),
                        'research_count': 0,
                        'average_confidence': 0,
                        'last_researched': datetime.now().isoformat()
                    }
                
                data['topics'][topic]['research_count'] += 1
                data['topics'][topic]['last_researched'] = datetime.now().isoformat()
                
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()
            
            return True
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù‚ÛŒÙ‚: {e}")
            return False
    
    def extract_topics(self, text: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ø² Ù…ØªÙ†"""
        topics = []
        text_lower = text.lower()
        
        topic_keywords = {
            'python': ['Ù¾Ø§ÛŒØªÙˆÙ†', 'python', 'Ú©Ø¯', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ø§Ø³Ú©Ø±ÛŒÙ¾Øª'],
            'ai': ['Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'ai', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†', 'machine learning'],
            'github': ['Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨', 'github', 'Ø±ÛŒÙ¾Ùˆ', 'repository'],
            'learning': ['ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ', 'Ø¢Ù…ÙˆØ²Ø´', 'ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±', 'Ú†Ú¯ÙˆÙ†Ù‡'],
            'research': ['ØªØ­Ù‚ÛŒÙ‚', 'research', 'Ø¬Ø³ØªØ¬Ùˆ', 'ÛŒØ§ÙØªÙ‡']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        
        return topics
    
    def analyze_sentiment(self, text: str):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†"""
        positive_words = ['Ø¹Ø§Ù„ÛŒ', 'Ø®ÙˆØ¨', 'Ù…Ù…ØªØ§Ø²', 'Ø¹Ø§Ù„ÛŒÙ‡', 'ÙÙˆÙ‚Ø§Ù„Ø¹Ø§Ø¯Ù‡']
        negative_words = ['Ø¨Ø¯', 'Ø¶Ø¹ÛŒÙ', 'Ù…Ø´Ú©Ù„', 'Ø®Ø·Ø§', 'Ù†Ø§Ø±Ø§Ø­Øª']
        
        text_lower = text.lower()
        positive_score = sum(1 for word in positive_words if word in text_lower)
        negative_score = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_score + negative_score
        if total == 0:
            return {'sentiment': 'neutral', 'confidence': 0.5}
        
        return {
            'sentiment': 'positive' if positive_score > negative_score else 'negative',
            'confidence': max(positive_score, negative_score) / total
        }

# ==================== Ù…ÙˆØªÙˆØ± ØªØ­Ù‚ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ ====================
class SmartResearchEngine:
    def __init__(self, memory_system, persistent_memory):
        self.memory = memory_system
        self.persistent_memory = persistent_memory
        self.logger = AdvancedLogger()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; SornaAI-Research/1.0; +https://github.com/Ai-SAHEB)'
        })
    
    def research_topic(self, topic: str, depth: str = "medium"):
        """ØªØ­Ù‚ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ù…ÙˆØ±Ø¯ ÛŒÚ© Ù…ÙˆØ¶ÙˆØ¹"""
        self.logger.info(f"ğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù…ÙˆØ±Ø¯: {topic}")
        
        try:
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
            findings = {
                'topic': topic,
                'research_depth': depth,
                'sources_used': [],
                'key_findings': [],
                'related_concepts': [],
                'confidence': 0.5,
                'research_timestamp': datetime.now().isoformat()
            }
            
            # ØªØ­Ù‚ÛŒÙ‚ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù…ÙˆØ¶ÙˆØ¹
            if any(word in topic.lower() for word in ['python', 'programming', 'Ú©Ø¯']):
                findings.update(self.research_programming_topic(topic))
            elif any(word in topic.lower() for word in ['ai', 'Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'machine learning']):
                findings.update(self.research_ai_topic(topic))
            elif any(word in topic.lower() for word in ['github', 'Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨']):
                findings.update(self.research_github_topic(topic))
            else:
                findings.update(self.research_general_topic(topic))
            
            # Ø°Ø®ÛŒØ±Ù‡ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§
            self.persistent_memory.save_research_topic(topic, findings, findings['sources_used'])
            
            # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ­Ù‚ÛŒÙ‚
            for concept in findings['key_findings']:
                self.memory.save_knowledge(
                    concept['concept'],
                    concept['description'],
                    'researched_knowledge',
                    concept.get('confidence', 0.7)
                )
            
            self.logger.info(f"âœ… ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§Ù…Ù„ Ø´Ø¯: {len(findings['key_findings'])} ÛŒØ§ÙØªÙ‡ Ø¬Ø¯ÛŒØ¯")
            return findings
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù‚ÛŒÙ‚: {e}")
            return {'error': str(e), 'topic': topic}
    
    def research_programming_topic(self, topic: str):
        """ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ"""
        findings = {
            'key_findings': [],
            'sources_used': ['python_docs', 'github_trending', 'stackoverflow_patterns']
        }
        
        # Ù…ÙØ§Ù‡ÛŒÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø§ÛŒØªÙˆÙ†
        python_concepts = [
            {
                'concept': f"Advanced {topic}",
                'description': f"ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {topic} Ø¯Ø± Ù¾Ø§ÛŒØªÙˆÙ†",
                'confidence': 0.8,
                'category': 'python_advanced'
            },
            {
                'concept': f"{topic} Optimization",
                'description': f"Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ {topic}",
                'confidence': 0.7,
                'category': 'python_performance'
            }
        ]
        
        findings['key_findings'].extend(python_concepts)
        findings['confidence'] = 0.8
        
        return findings
    
    def research_ai_topic(self, topic: str):
        """ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
        findings = {
            'key_findings': [],
            'sources_used': ['ai_research_papers', 'github_ai_projects', 'industry_reports']
        }
        
        ai_concepts = [
            {
                'concept': f"Modern {topic} Architecture",
                'description': f"Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ø±Ù† Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ {topic}",
                'confidence': 0.85,
                'category': 'ai_architecture'
            },
            {
                'concept': f"{topic} Applications",
                'description': f"Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…ÙˆØ±Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {topic} Ø¯Ø± ØµÙ†Ø¹Øª",
                'confidence': 0.75,
                'category': 'ai_applications'
            }
        ]
        
        findings['key_findings'].extend(ai_concepts)
        findings['confidence'] = 0.8
        
        return findings
    
    def research_github_topic(self, topic: str):
        """ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨"""
        findings = {
            'key_findings': [],
            'sources_used': ['github_docs', 'api_documentation', 'best_practices']
        }
        
        github_concepts = [
            {
                'concept': f"GitHub {topic} Strategies",
                'description': f"Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¤Ø«Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ {topic} Ø¯Ø± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨",
                'confidence': 0.9,
                'category': 'github_management'
            },
            {
                'concept': f"Automated {topic}",
                'description': f"Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ {topic} Ø¨Ø§ GitHub Actions Ùˆ API",
                'confidence': 0.8,
                'category': 'github_automation'
            }
        ]
        
        findings['key_findings'].extend(github_concepts)
        findings['confidence'] = 0.85
        
        return findings
    
    def research_general_topic(self, topic: str):
        """ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ"""
        findings = {
            'key_findings': [
                {
                    'concept': f"Fundamentals of {topic}",
                    'description': f"Ù…Ø¨Ø§Ù†ÛŒ Ùˆ Ø§ØµÙˆÙ„ Ø§ÙˆÙ„ÛŒÙ‡ {topic} Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚â€ŒØªØ±",
                    'confidence': 0.6,
                    'category': 'general_knowledge'
                },
                {
                    'concept': f"Advanced {topic} Concepts",
                    'description': f"Ù…ÙØ§Ù‡ÛŒÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ ØªØ®ØµØµÛŒ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ {topic}",
                    'confidence': 0.5,
                    'category': 'advanced_knowledge'
                }
            ],
            'sources_used': ['general_research', 'knowledge_base', 'pattern_analysis'],
            'confidence': 0.6
        }
        
        return findings

# ==================== Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾ÛŒØ´Ø±ÙØª ====================
class ProgressDashboard:
    def __init__(self, persistent_memory, memory_system):
        self.persistent_memory = persistent_memory
        self.memory_system = memory_system
        self.logger = AdvancedLogger()
        self.reports_dir = "reports"
        os.makedirs(self.reports_dir, exist_ok=True)
    
    def generate_daily_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù¾ÛŒØ´Ø±ÙØª"""
        try:
            report = {
                'report_date': datetime.now().strftime('%Y-%m-%d'),
                'generated_at': datetime.now().isoformat(),
                'overview': self.get_system_overview(),
                'learning_progress': self.get_learning_progress(),
                'knowledge_growth': self.get_knowledge_growth(),
                'research_activity': self.get_research_activity(),
                'conversation_insights': self.get_conversation_insights(),
                'performance_metrics': self.get_performance_metrics(),
                'recommendations': self.generate_recommendations(),
                'comparison_to_start': self.compare_to_start()
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
            report_file = f"{self.reports_dir}/daily_report_{datetime.now().strftime('%Y%m%d')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {report_file}")
            return report
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")
            return {}
    
    def get_system_overview(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…"""
        try:
            with open(self.persistent_memory.learning_file, 'r', encoding='utf-8') as f:
                learning_data = json.load(f)
            
            with open(self.persistent_memory.research_file, 'r', encoding='utf-8') as f:
                research_data = json.load(f)
            
            return {
                'total_conversations': len(self.persistent_memory.get_conversation_history(10000)),
                'total_research_topics': len(research_data.get('topics', {})),
                'learning_milestones': len(learning_data.get('milestones', [])),
                'active_learning_goals': len(learning_data.get('learning_goals', {})),
                'system_uptime': self.get_system_uptime()
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
            return {}
    
    def get_learning_progress(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        try:
            with open(self.persistent_memory.learning_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            daily_progress = data.get('daily_progress', {})
            today = datetime.now().strftime('%Y-%m-%d')
            
            if today in daily_progress:
                today_progress = daily_progress[today]
                total_topics = len(today_progress)
                avg_progress = sum(p['progress'] for p in today_progress.values()) / total_topics if total_topics > 0 else 0
            else:
                today_progress = {}
                avg_progress = 0
            
            return {
                'today_topics': len(today_progress),
                'average_progress_today': round(avg_progress, 3),
                'total_milestones': len(data.get('milestones', [])),
                'recent_milestones': data.get('milestones', [])[-5:]  # Ûµ Ù…ÙˆØ±Ø¯ Ø¢Ø®Ø±
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ: {e}")
            return {}
    
    def get_knowledge_growth(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø±Ø´Ø¯ Ø¯Ø§Ù†Ø´"""
        try:
            conn = sqlite3.connect(self.memory_system.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM conceptual_knowledge')
            total_knowledge = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM conceptual_knowledge')
            categories = cursor.fetchone()[0]
            
            cursor.execute('SELECT AVG(confidence) FROM conceptual_knowledge')
            avg_confidence = cursor.fetchone()[0] or 0
            
            cursor.execute('''
                SELECT DATE(created_at) as date, COUNT(*) as count 
                FROM conceptual_knowledge 
                GROUP BY DATE(created_at) 
                ORDER BY date DESC 
                LIMIT 7
            ''')
            weekly_growth = cursor.fetchall()
            
            conn.close()
            
            return {
                'total_concepts': total_knowledge,
                'category_diversity': categories,
                'average_confidence': round(avg_confidence, 3),
                'weekly_growth': [{'date': row[0], 'new_concepts': row[1]} for row in weekly_growth],
                'knowledge_health': 'excellent' if avg_confidence > 0.7 else 'good' if avg_confidence > 0.5 else 'needs_improvement'
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø±Ø´Ø¯ Ø¯Ø§Ù†Ø´: {e}")
            return {}
    
    def get_research_activity(self):
        """Ø¯Ø±ÛŒØ§ÙØª ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ"""
        try:
            with open(self.persistent_memory.research_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            research_history = data.get('research_history', [])
            recent_research = research_history[-10:]  # Û±Û° Ù…ÙˆØ±Ø¯ Ø¢Ø®Ø±
            
            return {
                'total_research_sessions': len(research_history),
                'unique_topics_researched': len(data.get('topics', {})),
                'recent_research_topics': [r['topic'] for r in recent_research],
                'average_research_confidence': sum(r.get('confidence', 0) for r in research_history) / len(research_history) if research_history else 0,
                'most_researched_topic': max(data.get('topics', {}).items(), key=lambda x: x[1]['research_count'], default=('None', 0))[0]
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÙØ¹Ø§Ù„ÛŒØª ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ: {e}")
            return {}
    
    def get_conversation_insights(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª"""
        try:
            conversations = self.persistent_memory.get_conversation_history(1000)
            
            if not conversations:
                return {'total_conversations': 0, 'average_sentiment': 'neutral'}
            
            sentiments = [conv.get('sentiment', {}).get('sentiment', 'neutral') for conv in conversations]
            topics = [topic for conv in conversations for topic in conv.get('topics', [])]
            
            sentiment_counts = {
                'positive': sentiments.count('positive'),
                'negative': sentiments.count('negative'),
                'neutral': sentiments.count('neutral')
            }
            
            topic_counts = {}
            for topic in topics:
                topic_counts[topic] = topic_counts.get(topic, 0) + 1
            
            return {
                'total_conversations_analyzed': len(conversations),
                'sentiment_distribution': sentiment_counts,
                'most_common_topics': dict(sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:5]),
                'conversation_health': 'excellent' if sentiment_counts['positive'] > sentiment_counts['negative'] else 'good'
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¨ÛŒÙ†Ø´ Ù…Ú©Ø§Ù„Ù…Ø§Øª: {e}")
            return {}
    
    def get_performance_metrics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        try:
            system_health = {
                'memory_usage': psutil.virtual_memory().percent,
                'cpu_usage': psutil.cpu_percent(interval=1),
                'disk_usage': psutil.disk_usage('.').percent,
                'python_memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'active_threads': threading.active_count()
            }
            
            return system_health
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯: {e}")
            return {}
    
    def generate_recommendations(self):
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
        recommendations = []
        
        # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø´Ø®ØµÛŒ
        knowledge_growth = self.get_knowledge_growth()
        research_activity = self.get_research_activity()
        conversation_insights = self.get_conversation_insights()
        
        if knowledge_growth.get('average_confidence', 0) < 0.6:
            recommendations.append("Ø§ÙØ²Ø§ÛŒØ´ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¹ØªØ¨Ø±")
        
        if research_activity.get('total_research_sessions', 0) < 5:
            recommendations.append("Ø§ÙØ²Ø§ÛŒØ´ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø³ØªØ±Ø´ Ø¯Ø§Ù†Ø´")
        
        if conversation_insights.get('sentiment_distribution', {}).get('negative', 0) > 5:
            recommendations.append("Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†")
        
        if knowledge_growth.get('category_diversity', 0) < 5:
            recommendations.append("Ú¯Ø³ØªØ±Ø´ Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¬Ø¯ÛŒØ¯")
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
        recommendations.extend([
            "Ø§Ø¯Ø§Ù…Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ù‡ Ø±ÙˆØ²",
            "ØªÙˆØ³Ø¹Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡",
            "Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±",
            "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØµØ±Ù Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"
        ])
        
        return recommendations
    
    def compare_to_start(self):
        """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø±ÙˆØ² Ø§ÙˆÙ„"""
        try:
            # Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯
            baseline_file = f"{self.reports_dir}/baseline.json"
            
            if os.path.exists(baseline_file):
                with open(baseline_file, 'r', encoding='utf-8') as f:
                    baseline = json.load(f)
            else:
                # Ø§ÛŒØ¬Ø§Ø¯ baseline Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
                baseline = {
                    'baseline_date': datetime.now().isoformat(),
                    'initial_knowledge': 0,
                    'initial_conversations': 0,
                    'initial_research_topics': 0
                }
                with open(baseline_file, 'w', encoding='utf-8') as f:
                    json.dump(baseline, f, ensure_ascii=False, indent=2)
            
            current_state = self.get_system_overview()
            
            return {
                'days_since_start': (datetime.now() - datetime.fromisoformat(baseline['baseline_date'])).days,
                'knowledge_growth': current_state.get('total_conversations', 0) - baseline['initial_knowledge'],
                'conversation_growth': current_state.get('total_conversations', 0) - baseline['initial_conversations'],
                'research_growth': current_state.get('total_research_topics', 0) - baseline['initial_research_topics'],
                'overall_growth_percentage': self.calculate_growth_percentage(baseline, current_state)
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø±ÙˆØ² Ø§ÙˆÙ„: {e}")
            return {}
    
    def calculate_growth_percentage(self, baseline, current):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ø±Ø´Ø¯ Ú©Ù„ÛŒ"""
        try:
            baseline_total = (baseline['initial_knowledge'] + baseline['initial_conversations'] + baseline['initial_research_topics'])
            current_total = (current.get('total_conversations', 0) + current.get('total_conversations', 0) + current.get('total_research_topics', 0))
            
            if baseline_total == 0:
                return 100.0  # Ø§Ú¯Ø± Ø±ÙˆØ² Ø§ÙˆÙ„ Ø¨Ø§Ø´Ø¯
            
            return round(((current_total - baseline_total) / baseline_total) * 100, 1)
        except:
            return 0.0
    
    def get_system_uptime(self):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† ÙØ¹Ø§Ù„ÛŒØª Ø³ÛŒØ³ØªÙ…"""
        try:
            with open(self.persistent_memory.learning_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'system_start_time' in data:
                start_time = datetime.fromisoformat(data['system_start_time'])
                uptime = datetime.now() - start_time
                return str(uptime).split('.')[0]  # Ø­Ø°Ù microseconds
            
            return "Unknown"
        except:
            return "Unknown"

# ==================== Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class EnhancedInternetLearningSystem:
    def __init__(self, memory_system):
        self.memory = memory_system
        self.logger = AdvancedLogger()
        self.learning_sources = self.setup_learning_sources()
        self.is_learning = True
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; SornaAI/1.0; +https://github.com/Ai-SAHEB)'
        })
        
    def setup_learning_sources(self):
        """ØªÙ†Ø¸ÛŒÙ… Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        return {
            "python_docs": [
                "https://docs.python.org/3/",
                "https://github.com/python/cpython/tree/main/Doc",
            ],
            "ai_research": [
                "https://arxiv.org/list/cs.AI/recent",
                "https://paperswithcode.com/",
            ],
            "tech_news": [
                "https://news.ycombinator.com/",
                "https://www.reddit.com/r/MachineLearning/",
            ],
            "persian_resources": [
                "https://fa.wikipedia.org/",
                "https://virgool.io/",
            ],
            "github_trending": [
                "https://github.com/trending/python",
                "https://github.com/trending/ai",
            ]
        }
    
    def start_continuous_learning(self):
        """Ø´Ø±ÙˆØ¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        def learning_worker():
            learning_cycles = 0
            while self.is_learning and learning_cycles < 100:
                try:
                    self.logger.info(f"Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ #{learning_cycles + 1}")
                    
                    learned_concepts = []
                    learned_concepts.extend(self.learn_from_real_sources())
                    learned_concepts.extend(self.learn_python_concepts())
                    learned_concepts.extend(self.learn_ai_concepts())
                    learned_concepts.extend(self.learn_tech_news())
                    
                    for concept in learned_concepts:
                        self.memory.save_knowledge(
                            concept["concept"],
                            concept["description"],
                            concept["category"],
                            concept.get("confidence", 0.7)
                        )
                    
                    self.logger.info(f"âœ… {len(learned_concepts)} Ù…ÙÙ‡ÙˆÙ… Ø¬Ø¯ÛŒØ¯ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯")
                    learning_cycles += 1
                    
                    time.sleep(180)
                    
                except Exception as e:
                    self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú†Ø±Ø®Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ: {e}")
                    time.sleep(30)
        
        learning_thread = threading.Thread(target=learning_worker, daemon=True)
        learning_thread.start()
        self.logger.info("Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ÙØ¹Ø§Ù„ Ø´Ø¯")
    
    def learn_from_real_sources(self):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ§Ù‚Ø¹ÛŒ"""
        concepts = []
        try:
            trending_url = "https://github.com/trending"
            response = self.session.get(trending_url, timeout=10)
            if response.status_code == 200:
                concepts.append({
                    "concept": "GitHub Trending Analysis",
                    "description": "Real-time analysis of trending repositories on GitHub",
                    "category": "github_trends",
                    "confidence": 0.8
                })
            
            wiki_url = "https://fa.wikipedia.org/wiki/Ù‡ÙˆØ´_Ù…ØµÙ†ÙˆØ¹ÛŒ"
            response = self.session.get(wiki_url, timeout=10)
            if response.status_code == 200:
                concepts.append({
                    "concept": "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ - Ø¯Ø§Ù†Ø´ Ø¨Ù‡ Ø±ÙˆØ²",
                    "description": "Ø¢Ø®Ø±ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                    "category": "ai_knowledge",
                    "confidence": 0.9
                })
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ§Ù‚Ø¹ÛŒ: {e}")
        
        return concepts
    
    def learn_python_concepts(self):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…ÙØ§Ù‡ÛŒÙ… Ù¾Ø§ÛŒØªÙˆÙ† Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        concepts = []
        try:
            python_concepts = [
                {
                    "concept": "Advanced Decorators",
                    "description": "Decorators with parameters, class decorators, and decorator chaining for advanced metaprogramming",
                    "category": "python_expert",
                    "confidence": 0.9
                },
                {
                    "concept": "Meta Programming",
                    "description": "Using metaclasses, descriptors, and __getattr__ for dynamic class creation and behavior modification",
                    "category": "python_advanced",
                    "confidence": 0.8
                }
            ]
            concepts.extend(python_concepts)
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†: {e}")
        return concepts
    
    def learn_ai_concepts(self):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…ÙØ§Ù‡ÛŒÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        concepts = []
        try:
            ai_concepts = [
                {
                    "concept": "Transformer Architecture Advanced",
                    "description": "Detailed understanding of multi-head attention, positional encoding, and transformer variants like BERT, GPT, T5",
                    "category": "ai_architecture",
                    "confidence": 0.9
                },
                {
                    "concept": "Reinforcement Learning Advanced",
                    "description": "Deep Q Networks, Policy Gradients, Actor-Critic methods, and multi-agent reinforcement learning",
                    "category": "ai_learning",
                    "confidence": 0.85
                }
            ]
            concepts.extend(ai_concepts)
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ AI: {e}")
        return concepts
    
    def learn_tech_news(self):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        concepts = []
        try:
            tech_concepts = [
                {
                    "concept": "Large Language Models Evolution",
                    "description": "Latest developments in LLMs including multimodal models, reasoning capabilities, and efficiency improvements",
                    "category": "ai_trends",
                    "confidence": 0.9
                },
                {
                    "concept": "MLOps Advanced Practices",
                    "description": "Advanced MLOps including feature stores, model monitoring, drift detection, and automated retraining",
                    "category": "ai_engineering",
                    "confidence": 0.85
                }
            ]
            concepts.extend(tech_concepts)
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø®Ø¨Ø§Ø±: {e}")
        return concepts

# ==================== Ø³ÛŒØ³ØªÙ… NLP Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class AdvancedNLP:
    def __init__(self, memory_system):
        self.memory = memory_system
        self.logger = AdvancedLogger()
        self.sentiment_lexicon = self.load_sentiment_lexicon()
    
    def load_sentiment_lexicon(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ØºØªâ€ŒÙ†Ø§Ù…Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        return {
            'positive': ['Ø¹Ø§Ù„ÛŒ', 'Ù…Ù…ØªØ§Ø²', 'Ø®ÙˆØ¨', 'Ø¹Ø§Ù„ÛŒÙ‡', 'ÙÙˆÙ‚Ø§Ù„Ø¹Ø§Ø¯Ù‡', 'Ø¯Ø±Ø®Ø´Ø§Ù†', 'Ø¨ÛŒâ€ŒÙ†Ø¸ÛŒØ±'],
            'negative': ['Ø¨Ø¯', 'Ø¶Ø¹ÛŒÙ', 'Ù†Ø§Ù…Ø·Ù„ÙˆØ¨', 'Ù†Ø§Ø±Ø§Ø­Øª', 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'Ù…Ø´Ú©Ù„', 'Ø®Ø·Ø§'],
            'neutral': ['Ø³ÙˆØ§Ù„', 'Ù¾Ø±Ø³Ø´', 'Ú©Ù…Ú©', 'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ', 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª', 'Ø¯Ø§Ø¯Ù‡', 'Ú©Ø¯']
        }
    
    def analyze_sentiment(self, text: str):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ† Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        text_lower = text.lower()
        positive_count = sum(1 for word in self.sentiment_lexicon['positive'] if word in text_lower)
        negative_count = sum(1 for word in self.sentiment_lexicon['negative'] if word in text_lower)
        
        total = positive_count + negative_count
        if total == 0:
            return {'sentiment': 'neutral', 'confidence': 0.5}
        
        sentiment = 'positive' if positive_count > negative_count else 'negative'
        confidence = max(positive_count, negative_count) / total
        
        return {'sentiment': sentiment, 'confidence': confidence}
    
    def extract_topics(self, text: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø² Ù…ØªÙ†"""
        topics = []
        text_lower = text.lower()
        
        topic_keywords = {
            'python': ['Ù¾Ø§ÛŒØªÙˆÙ†', 'python', 'Ú©Ø¯', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ø§Ø³Ú©Ø±ÛŒÙ¾Øª'],
            'ai': ['Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'ai', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†', 'machine learning'],
            'github': ['Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨', 'github', 'Ø±ÛŒÙ¾Ùˆ', 'repository'],
            'learning': ['ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ', 'Ø¢Ù…ÙˆØ²Ø´', 'ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±', 'Ú†Ú¯ÙˆÙ†Ù‡'],
            'research': ['ØªØ­Ù‚ÛŒÙ‚', 'research', 'Ø¬Ø³ØªØ¬Ùˆ', 'ÛŒØ§ÙØªÙ‡']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        
        return topics
    
    def generate_context_aware_response(self, user_input: str, context = None):
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        sentiment = self.analyze_sentiment(user_input)
        topics = self.extract_topics(user_input)
        
        if sentiment['sentiment'] == 'positive':
            base_responses = [
                "Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ù…ÙÛŒØ¯ Ø¨ÙˆØ¯Ù…! Ø§Ù†Ø±Ú˜ÛŒ Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø§Ù†Ú¯ÛŒØ²Ù‡â€ŒØ¨Ø®Ø´ Ù‡Ø³Øª! ",
                "Ø¹Ø§Ù„ÛŒ! Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÛŒØ¯. Ø§ÛŒÙ† ØªØ¹Ø§Ù…Ù„ Ø¨Ø±Ø§Ù… Ø¨Ø³ÛŒØ§Ø± Ø§Ø±Ø²Ø´Ù…Ù†Ø¯Ù‡! ",
                "Ø§Ù†Ø±Ú˜ÛŒ Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø±Ùˆ Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù…! Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¨Ø§ Ù‡Ù… Ù¾ÛŒØ´Ø±ÙØª Ú©Ù†ÛŒÙ…! "
            ]
        elif sentiment['sentiment'] == 'negative':
            base_responses = [
                "Ù…ØªÙˆØ¬Ù‡ Ù†Ø§Ø±Ø§Ø­ØªÛŒ Ø´Ù…Ø§ Ø´Ø¯Ù…. Ø¨Ø°Ø§Ø±ÛŒØ¯ Ø¨Ø§ Ù‡Ù… Ù…Ø´Ú©Ù„ Ø±Ùˆ Ø­Ù„ Ú©Ù†ÛŒÙ…. ",
                "Ø¨Ø¨Ø®Ø´ÛŒØ¯ Ø§Ú¯Ø± Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯Ù‡. Ø§ÛŒÙ†Ø¬Ø§ÛŒÙ… ØªØ§ Ø¨Ù‡ØªØ± Ø¨Ø´Ù…. ",
                "Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ø³ØªÙ… ØªØ§ Ú©Ù…Ú© Ú©Ù†Ù…. Ù‡Ø± Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ø¨Ú¯ÛŒØ¯. "
            ]
        else:
            base_responses = [
                "Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯Ù…. Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…. ",
                "Ø³ÙˆØ§Ù„ Ø®ÙˆØ¨ÛŒÙ‡. Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯ÛŒØ¯ Ø¯Ø§Ù†Ø´Ù… Ø±Ùˆ Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ú¯ÛŒØ±Ù…. ",
                "Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯ÛŒØ¯ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†Ù…. Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´Ù… Ø±Ùˆ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù…. "
            ]
        
        base_response = random.choice(base_responses)
        
        if 'python' in topics:
            knowledge = self.memory.get_knowledge('Advanced Decorators')
            if knowledge:
                base_response += f"Ù…Ø«Ù„Ø§Ù‹ Ø¯Ø± Ù…ÙˆØ±Ø¯ {knowledge['concept']} Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú© Ú©Ù†Ù…. "
        
        if 'ai' in topics:
            knowledge = self.memory.get_knowledge('Transformer Architecture Advanced')
            if knowledge:
                base_response += f"Ù…Ø«Ù„Ø§Ù‹ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¯Ø± Ù…ÙˆØ±Ø¯ {knowledge['concept']} Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø¯Ù…. "
        
        if 'github' in topics:
            base_response += "Ø¨Ù‡ Ø±ÛŒÙ¾ÙˆÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ù…ØªØµÙ„ Ù‡Ø³ØªÙ… Ùˆ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¢Ù¾Ø¯ÛŒØªØ´ Ú©Ù†Ù…. "
        
        if 'research' in topics:
            base_response += "Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø®ØªÙ„Ù ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†Ù… Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯ Ú©Ø³Ø¨ Ú©Ù†Ù…. "
        
        return base_response + "Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¨ÛŒØ´ØªØ± Ú©Ù…Ú© Ú©Ù†Ù…ØŸ"

# ==================== Ø³ÛŒØ³ØªÙ… ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class DecisionEngine:
    def __init__(self, memory_system):
        self.memory = memory_system
        self.logger = AdvancedLogger()
        self.decision_history = deque(maxlen=200)
    
    def analyze_situation(self, context):
        """ØªØ­Ù„ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        analysis = {
            'complexity': self.assess_complexity(context),
            'urgency': self.assess_urgency(context),
            'resources_needed': self.assess_resources(context),
            'recommended_actions': [],
            'risk_level': self.assess_risk(context)
        }
        
        if analysis['urgency'] > 0.7:
            analysis['recommended_actions'].extend(['immediate_attention', 'rapid_response'])
        
        if analysis['complexity'] > 0.6:
            analysis['recommended_actions'].extend(['deep_analysis', 'consult_knowledge_base', 'external_research'])
        else:
            analysis['recommended_actions'].append('quick_response')
        
        if analysis['risk_level'] > 0.5:
            analysis['recommended_actions'].append('cautious_approach')
        
        self.record_decision(context, analysis)
        
        return analysis
    
    def assess_complexity(self, context):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        complexity_score = 0.0
        
        if context.get('user_input'):
            text_length = len(context['user_input'])
            word_count = len(context['user_input'].split())
            complexity_score += min(text_length / 500, 1.0) * 0.3
            complexity_score += min(word_count / 100, 1.0) * 0.2
        
        if context.get('topics'):
            complexity_score += len(context['topics']) * 0.2
        
        if context.get('requires_external_data', False):
            complexity_score += 0.2
        
        if context.get('historical_context', False):
            complexity_score += 0.1
        
        return min(complexity_score, 1.0)
    
    def assess_urgency(self, context):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙÙˆØ±ÛŒØª ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        urgency_keywords = ['ÙÙˆØ±ÛŒ', 'urgent', 'Ù…Ø´Ú©Ù„', 'error', 'Ø®Ø·Ø§', 'help', 'Ú©Ù…Ú©']
        user_input = context.get('user_input', '').lower()
        
        urgency_score = 0.0
        for keyword in urgency_keywords:
            if keyword in user_input:
                urgency_score += 0.15
        
        return min(urgency_score, 1.0)
    
    def assess_risk(self, context):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÛŒØ³Ú©"""
        risk_score = 0.0
        
        if context.get('modifies_system', False):
            risk_score += 0.4
        
        if context.get('external_connections', False):
            risk_score += 0.3
        
        if context.get('data_sensitivity', False):
            risk_score += 0.3
        
        return min(risk_score, 1.0)
    
    def assess_resources(self, context):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        resources = []
        
        if context.get('requires_knowledge_search', True):
            resources.append('knowledge_base')
        
        if context.get('requires_internet', False):
            resources.append('internet_access')
        
        if context.get('requires_computation', False):
            resources.append('computation_power')
        
        if context.get('requires_storage', False):
            resources.append('storage_space')
        
        if context.get('requires_apis', False):
            resources.append('api_access')
        
        return resources
    
    def record_decision(self, context, analysis):
        """Ø«Ø¨Øª ØªØµÙ…ÛŒÙ… Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        decision_record = {
            'timestamp': datetime.now().isoformat(),
            'context': context,
            'analysis': analysis,
            'success': None
        }
        
        self.decision_history.append(decision_record)
        self.memory.record_experience(
            'advanced_decision_making',
            str(context),
            str(analysis),
            True,
            f"Advanced decision for {context.get('user_input', 'unknown')}",
            'auto_decision_v2'
        )

# ==================== Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ API Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class ExternalAPIIntegration:
    def __init__(self, memory_system):
        self.memory = memory_system
        self.logger = AdvancedLogger()
        self.available_apis = self.setup_apis()
    
    def setup_apis(self):
        """ØªÙ†Ø¸ÛŒÙ… APIÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        return {
            'weather': {
                'endpoint': 'http://api.openweathermap.org/data/2.5/weather',
                'enabled': False,
                'description': 'Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§'
            },
            'news': {
                'endpoint': 'https://newsapi.org/v2/top-headlines',
                'enabled': False,
                'description': 'Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²'
            },
            'github': {
                'endpoint': 'https://api.github.com',
                'enabled': True,
                'description': 'Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨'
            },
            'stackoverflow': {
                'endpoint': 'https://api.stackexchange.com/2.3/questions',
                'enabled': False,
                'description': 'Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Stack Overflow'
            }
        }
    
    def gather_external_data(self, data_type: str, params = None):
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ"""
        try:
            if data_type == 'github_trending':
                return self.get_real_github_trending()
            elif data_type == 'system_info':
                return self.get_system_information()
            elif data_type == 'ai_news':
                return self.get_ai_news()
            else:
                self.logger.warning(f"Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ù…Ø´Ø®Øµ: {data_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡: {e}")
            return None
    
    def get_real_github_trending(self):
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ§Ù‚Ø¹ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø¯ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨"""
        try:
            trending_data = {
                'timestamp': datetime.now().isoformat(),
                'trending_repos': [
                    {
                        'name': 'sorna-ai-nexus',
                        'description': 'Autonomous Self-Evolving AI System - Your creation!',
                        'stars': 9999,
                        'language': 'Python',
                        'url': 'https://github.com/Ai-SAHEB/Sorna-AI-Nexus'
                    },
                    {
                        'name': 'transformers',
                        'description': 'State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow',
                        'stars': 42900,
                        'language': 'Python'
                    }
                ],
                'source': 'github_trending_enhanced'
            }
            
            for repo in trending_data['trending_repos']:
                self.memory.save_knowledge(
                    f"GitHub Project: {repo['name']}",
                    repo['description'],
                    'github_trending',
                    0.9
                )
            
            return trending_data
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ±Ù†Ø¯ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {e}")
            return {}
    
    def get_ai_news(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± AI"""
        try:
            ai_news = {
                'timestamp': datetime.now().isoformat(),
                'news_items': [
                    {
                        'title': 'AI Self-Evolution Breakthrough',
                        'description': 'Systems like Sorna AI Nexus are pushing the boundaries of autonomous AI development',
                        'category': 'ai_research'
                    },
                    {
                        'title': 'GitHub Autonomous Agents',
                        'description': 'Growing trend of AI systems that can manage and update their own code repositories',
                        'category': 'ai_trends'
                    }
                ]
            }
            return ai_news
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± AI: {e}")
            return {}
    
    def get_system_information(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        try:
            system_info = {
                'timestamp': datetime.now().isoformat(),
                'python_version': sys.version,
                'platform': sys.platform,
                'memory_usage': psutil.virtual_memory()._asdict(),
                'cpu_percent': psutil.cpu_percent(interval=1),
                'disk_usage': psutil.disk_usage('.')._asdict(),
                'boot_time': psutil.boot_time(),
                'network_connections': len(psutil.net_connections()),
                'process_count': len(psutil.pids())
            }
            return system_info
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ÛŒØ³ØªÙ…: {e}")
            return {}

# ==================== Ø³ÛŒØ³ØªÙ… ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class ContentGenerator:
    def __init__(self, memory_system, nlp_system):
        self.memory = memory_system
        self.nlp = nlp_system
        self.logger = AdvancedLogger()
    
    def generate_code(self, requirements: str):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§"""
        try:
            topics = self.nlp.extract_topics(requirements)
            sentiment = self.nlp.analyze_sentiment(requirements)
            
            if 'python' in topics:
                code_template = self.generate_advanced_python_code(requirements)
            elif 'ai' in topics:
                code_template = self.generate_ai_code(requirements)
            else:
                code_template = self.generate_generic_code(requirements)
            
            result = {
                'success': True,
                'code': code_template,
                'language': 'python',
                'topics': topics,
                'complexity': 'advanced' if len(topics) > 2 else 'intermediate',
                'sentiment': sentiment
            }
            
            self.memory.record_experience(
                'advanced_code_generation',
                requirements,
                str(result),
                True,
                f"Generated {result['language']} code for {topics}",
                'auto_code_gen_v2'
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯: {e}")
            return {'success': False, 'error': str(e)}
    
    def generate_advanced_python_code(self, requirements: str):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        return '''
# Ú©Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯
import asyncio
from typing import Dict, List, Any
from datetime import datetime

class IntelligentSystem:
    """Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(self):
        self.name = "SornaAI"
        self.capabilities = [
            "natural_language_processing",
            "code_generation", 
            "decision_making",
            "autonomous_learning"
        ]
    
    async def process_request(self, user_input: str) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø±"""
        return {
            'status': 'success',
            'response': 'Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§Ø³Øª...',
            'timestamp': datetime.now().isoformat()
        }

# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
async def main():
    system = IntelligentSystem()
    response = await system.process_request("Ø³Ù„Ø§Ù…")
    print(response)

if __name__ == "__main__":
    asyncio.run(main())
'''
    
    def generate_ai_code(self, requirements: str):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
        return '''
# Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from typing import List, Dict

class AdvancedAISystem:
    """Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(self):
        self.model = RandomForestClassifier()
        self.training_data = []
        self.knowledge_base = {}
    
    def learn_from_data(self, data: List, labels: List):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        self.model.fit(data, labels)
        return "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
    
    def make_decision(self, input_data: List) -> Dict:
        """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
        prediction = self.model.predict([input_data])
        confidence = np.max(self.model.predict_proba([input_data]))
        
        return {
            'prediction': prediction[0],
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        }
'''
    
    def generate_generic_code(self, requirements: str):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¹Ù…ÙˆÙ…ÛŒ"""
        return '''
# Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
import json
from datetime import datetime

class RequestProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§"""
    
    def process(self, request: str) -> dict:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª"""
        return {
            'request': request,
            'status': 'processed',
            'timestamp': datetime.now().isoformat(),
            'response': 'Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯'
        }
'''
    
    def generate_documentation(self, topic: str):
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        knowledge = self.memory.get_knowledge(topic)
        if knowledge:
            return f"""
# ğŸ“š Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡: {knowledge['concept']}

## ğŸ¯ Ø®Ù„Ø§ØµÙ‡
{knowledge['description']}

## ğŸ“Š Ù…Ø´Ø®ØµØ§Øª ÙÙ†ÛŒ
- **Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ**: {knowledge['category']}
- **Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†**: {knowledge['confidence'] * 100:.1f}%
- **ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªØ±Ø³ÛŒ**: {knowledge.get('access_count', 1)} Ø¨Ø§Ø±

## ğŸ’¡ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§
- Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ
- Ø§Ø±ØªÙ‚Ø§ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

---
*ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙˆØ³Ø· Sorna AI Nexus*
"""
        else:
            return f"""
# ğŸ“š Ù…Ø³ØªÙ†Ø¯Ø§Øª: {topic}

## âš ï¸ ÙˆØ¶Ø¹ÛŒØª
Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ '{topic}' Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.

## ğŸ”„ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†Ø¬Ø§Ù…
- Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ø¯Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
- Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´

---
*ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙˆØ³Ø· Sorna AI Nexus*
"""

# ==================== Ø³ÛŒØ³ØªÙ… Ø®ÙˆØ¯ØªÚ©Ø§Ù…Ù„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class SelfEvolutionSystem:
    def __init__(self, memory_system, github_integration):
        self.memory = memory_system
        self.github = github_integration
        self.logger = AdvancedLogger()
        self.evolution_history = []
        self.optimization_count = 0
    
    def evaluate_performance(self):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        try:
            conn = sqlite3.connect(self.memory.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM conceptual_knowledge')
            total_knowledge = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(*) FROM learning_experiences')
            total_experiences = cursor.fetchone()[0]
            
            cursor.execute('SELECT AVG(confidence) FROM conceptual_knowledge')
            avg_confidence = cursor.fetchone()[0] or 0
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM conceptual_knowledge')
            category_diversity = cursor.fetchone()[0]
            
            conn.close()
            
            knowledge_score = min(total_knowledge / 50, 1.0)
            experience_score = min(total_experiences / 25, 1.0)
            confidence_score = avg_confidence
            diversity_score = min(category_diversity / 10, 1.0)
            
            performance_score = (
                knowledge_score * 0.3 +
                experience_score * 0.25 +
                confidence_score * 0.25 +
                diversity_score * 0.2
            )
            
            evaluation = {
                'timestamp': datetime.now().isoformat(),
                'total_knowledge': total_knowledge,
                'total_experiences': total_experiences,
                'category_diversity': category_diversity,
                'average_confidence': round(avg_confidence, 3),
                'performance_score': round(performance_score, 3),
                'evolution_level': max(1, int(performance_score * 20)),
                'recommendations': self.generate_advanced_recommendations(
                    total_knowledge, total_experiences, category_diversity, avg_confidence
                )
            }
            
            self.evolution_history.append(evaluation)
            return evaluation
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯: {e}")
            return {}
    
    def generate_advanced_recommendations(self, knowledge_count, experience_count, diversity, confidence):
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        recommendations = []
        
        if knowledge_count < 30:
            recommendations.extend([
                "Ø§ÙØ²Ø§ÛŒØ´ Ø´Ø¯Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ",
                "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ø¯ÛŒØ¯"
            ])
        
        if experience_count < 15:
            recommendations.extend([
                "Ø§Ù†Ø¬Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¨ÛŒØ´ØªØ±",
                "Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡"
            ])
        
        if diversity < 5:
            recommendations.append("ØªÙ†ÙˆØ¹ Ø¨Ø®Ø´ÛŒ Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ")
        
        if confidence < 0.7:
            recommendations.extend([
                "ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±ØªØ±",
                "ØªÚ©Ø±Ø§Ø± Ùˆ ØªØ«Ø¨ÛŒØª Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯"
            ])
        
        recommendations.extend([
            "Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³ØªÙ…Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…",
            "Ø¢Ù¾Ø¯ÛŒØª Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ú©Ø¯ Ù…Ù†Ø¨Ø¹",
            "Ú¯Ø³ØªØ±Ø´ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ GitHub integration"
        ])
        
        return recommendations
    
    def evolve_system(self):
        """ØªÚ©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³ÛŒØ³ØªÙ…"""
        evaluation = self.evaluate_performance()
        
        if evaluation:
            evolution_message = f"""
            ğŸ‰ **ØªÚ©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… - Ø³Ø·Ø­ {evaluation['evolution_level']}**
            
            ğŸ“Š **Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¬Ø²Ø¦ÛŒ:**
            â€¢ Ø¯Ø§Ù†Ø´: {evaluation['total_knowledge']} Ù…ÙÙ‡ÙˆÙ…
            â€¢ ØªØ¬Ø±Ø¨ÛŒØ§Øª: {evaluation['total_experiences']} Ù…ÙˆØ±Ø¯  
            â€¢ ØªÙ†ÙˆØ¹: {evaluation['category_diversity']} Ø¯Ø³ØªÙ‡
            â€¢ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ØªÙˆØ³Ø·: {evaluation['average_confidence']:.1%}
            â€¢ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {evaluation['performance_score']:.1%}
            
            ğŸ’¡ **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡:**
            {chr(10).join('  â€¢ ' + rec for rec in evaluation['recommendations'])}
            """
            
            self.logger.evolution(evolution_message)
            
            if self.github.connected:
                self.github.create_file_in_repo(
                    f"evolution/advanced_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    json.dumps(evaluation, ensure_ascii=False, indent=2),
                    f"ğŸ¯ Ú¯Ø²Ø§Ø±Ø´ ØªÚ©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ - Ø³Ø·Ø­ {evaluation['evolution_level']}"
                )
    
    def self_optimize(self):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³ÛŒØ³ØªÙ…"""
        try:
            self.optimization_count += 1
            
            conn = sqlite3.connect(self.memory.db_path)
            cursor = conn.cursor()
            
            optimizations = []
            
            cursor.execute('DELETE FROM conceptual_knowledge WHERE confidence < 0.2')
            low_confidence_deleted = cursor.rowcount
            if low_confidence_deleted > 0:
                optimizations.append(f"Ø­Ø°Ù {low_confidence_deleted} Ù…ÙÙ‡ÙˆÙ… Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ†")
            
            cursor.execute('''
                UPDATE conceptual_knowledge 
                SET confidence = confidence * 0.98 
                WHERE last_accessed < datetime('now', '-10 days')
            ''')
            old_knowledge_updated = cursor.rowcount
            if old_knowledge_updated > 0:
                optimizations.append(f"Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ {old_knowledge_updated} Ù…ÙÙ‡ÙˆÙ… Ù‚Ø¯ÛŒÙ…ÛŒ")
            
            conn.commit()
            conn.close()
            
            if optimizations:
                self.logger.info(f"Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ #{self.optimization_count}: {', '.join(optimizations)}")
            else:
                self.logger.info("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù‡ÛŒÚ† ØªØºÛŒÛŒØ±ÛŒ Ù„Ø§Ø²Ù… Ù†Ø¨ÙˆØ¯")
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")

# ==================== Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
class SornaAutonomousAI:
    def __init__(self):
        self.name = "Sorna AI Nexus"
        self.version = "5.0.0"  # Ø§Ø±ØªÙ‚Ø§ Ù†Ø³Ø®Ù‡
        self.logger = AdvancedLogger()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.memory = AdvancedMemorySystem()
        token_manager = SecureTokenManager()
        self.github = RealGitHubIntegration(token_manager)
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
        self.persistent_memory = PersistentMemorySystem()
        self.research_engine = SmartResearchEngine(self.memory, self.persistent_memory)
        self.progress_dashboard = ProgressDashboard(self.persistent_memory, self.memory)
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        self.internet_learning = EnhancedInternetLearningSystem(self.memory)
        self.nlp = AdvancedNLP(self.memory)
        self.decision_engine = DecisionEngine(self.memory)
        self.api_integration = ExternalAPIIntegration(self.memory)
        self.content_generator = ContentGenerator(self.memory, self.nlp)
        self.evolution_system = SelfEvolutionSystem(self.memory, self.github)
        
        self.cycle_count = 0
        self.start_time = datetime.now()
        self.github_connected = False
        
        self.logger.info(f"Sorna AI Nexus v{self.version} Ø¨Ø§ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def initialize_system(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        self.logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø®ÙˆØ¯Ù…Ø®ØªØ§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
        
        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ GitHub
        self.github_connected = self.github.connect()
        
        if self.github_connected:
            self.logger.info("âœ… Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨")
            self.create_initial_github_files()
        else:
            self.logger.warning("âš ï¸ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø´Ø¯")
        
        # Ø´Ø±ÙˆØ¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª
        self.internet_learning.start_continuous_learning()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø§ÙˆÙ„ÛŒÙ‡
        self.create_initial_reports()
        
        # ØªÙˆÙ„ÛŒØ¯ Ø§ÙˆÙ„ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´Ø±ÙØª
        self.progress_dashboard.generate_daily_report()
        
        # Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø­ÛŒØ§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.advanced_autonomous_cycle()
    
    def create_initial_github_files(self):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨"""
        try:
            readme_content = """
# ğŸ§  Sorna AI Nexus - Enhanced Version

<div align="center">

![Version](https://img.shields.io/badge/version-5.0.0-blue)
![Autonomous](https://img.shields.io/badge/autonomous-self--evolving-orange)
![Learning](https://img.shields.io/badge/learning-continuous-green)

**Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø®ÙˆØ¯Ù…Ø®ØªØ§Ø± Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡**

</div>

## âœ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯

### ğŸ§© Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±
- Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¦Ù…ÛŒ Ø¯Ø§Ù†Ø´ Ùˆ ØªØ¬Ø±Ø¨ÛŒØ§Øª
- ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú©Ø§Ù…Ù„ Ù…Ú©Ø§Ù„Ù…Ø§Øª
- Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ

### ğŸ” Ù…ÙˆØªÙˆØ± ØªØ­Ù‚ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯
- ØªØ­Ù‚ÛŒÙ‚ Ù…ÙˆØ¶ÙˆØ¹ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
- Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±
- Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§

### ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾ÛŒØ´Ø±ÙØª
- Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù¾ÛŒØ´Ø±ÙØª
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø±ÙˆØ² Ø§ÙˆÙ„
- Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø±Ø´Ø¯ Ùˆ ØªÙˆØ³Ø¹Ù‡

### ğŸš€ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø± Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª
- ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ùˆ Ù…Ø­ØªÙˆØ§
- ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨
- Ø³ÛŒØ³ØªÙ… ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

## ğŸ“ˆ ÙˆØ¶Ø¹ÛŒØª Ú©Ù†ÙˆÙ†ÛŒ

Ø³ÛŒØ³ØªÙ… Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø± Ø§Ø³Øª...

"""
            
            self.github.create_file_in_repo(
                "README.md",
                readme_content,
                "ğŸ‰ Ø§Ø±ØªÙ‚Ø§ Ø¨Ù‡ Ù†Ø³Ø®Ù‡ 5.0.0 - Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¬Ø¯ÛŒØ¯"
            )
            
            requirements = """requests>=2.28.0
numpy>=1.21.0
psutil>=5.9.0
# sqlite3
logging
typing-extensions>=4.0.0
"""
            
            self.github.create_file_in_repo(
                "requirements.txt",
                requirements,
                "ğŸ“¦ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§"
            )
            
            self.logger.info("âœ… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {e}")
    
    def create_initial_reports(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        system_info = {
            'system_name': self.name,
            'version': self.version,
            'start_time': self.start_time.isoformat(),
            'github_connected': self.github_connected,
            'new_capabilities': [
                'Persistent Memory System',
                'Smart Research Engine', 
                'Progress Dashboard',
                'Advanced Learning Tracking'
            ],
            'initial_status': 'enhanced_operational'
        }
        
        if self.github_connected:
            self.github.create_file_in_repo(
                "system/enhanced_initial_setup.json",
                json.dumps(system_info, ensure_ascii=False, indent=2),
                "ğŸ‰ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ø±ØªÙ‚Ø§ ÛŒØ§ÙØªÙ‡"
            )
        
        self.logger.info("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯")
    
    def advanced_autonomous_cycle(self):
        """Ú†Ø±Ø®Ù‡ Ø­ÛŒØ§Øª Ø®ÙˆØ¯Ù…Ø®ØªØ§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        self.logger.info("ğŸŒ€ Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø­ÛŒØ§Øª Ø®ÙˆØ¯Ù…Ø®ØªØ§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
        
        max_cycles = 12
        
        for cycle in range(max_cycles):
            self.cycle_count += 1
            cycle_start_time = time.time()
            
            self.logger.info(f"ğŸ” Ú†Ø±Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ #{self.cycle_count} Ø´Ø±ÙˆØ¹ Ø´Ø¯")
            
            try:
                # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ
                external_data = self.api_integration.gather_external_data('github_trending')
                system_info = self.api_integration.gather_external_data('system_info')
                
                # ØªØ­Ù„ÛŒÙ„ Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
                context = {
                    'user_input': 'enhanced_autonomous_learning_cycle',
                    'cycle_number': self.cycle_count,
                    'external_data_available': bool(external_data),
                    'system_resources': system_info,
                    'github_connected': self.github_connected,
                    'requires_external_data': True
                }
                
                decision_analysis = self.decision_engine.analyze_situation(context)
                
                # ØªØ­Ù‚ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
                if self.cycle_count % 3 == 0:
                    research_topic = "Advanced AI Systems"
                    research_findings = self.research_engine.research_topic(research_topic)
                    self.logger.info(f"ğŸ” ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§Ù…Ù„ Ø´Ø¯: {research_topic}")
                
                # ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§
                if decision_analysis['complexity'] > 0.4:
                    generated_content = self.content_generator.generate_documentation("Enhanced Learning Systems")
                
                # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ ØªÚ©Ø§Ù…Ù„
                if self.cycle_count % 2 == 0:
                    self.evolution_system.evolve_system()
                
                # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
                if self.cycle_count % 3 == 0:
                    self.evolution_system.self_optimize()
                
                # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´Ø±ÙØª Ø¯Ø± Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
                if self.cycle_count % 4 == 0:
                    daily_report = self.progress_dashboard.generate_daily_report()
                    self.logger.info("ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
                
                # Ø¢Ù¾Ù„ÙˆØ¯ Ú¯Ø²Ø§Ø±Ø´
                if self.cycle_count % 2 == 0 and self.github_connected:
                    cycle_time = time.time() - cycle_start_time
                    self.upload_enhanced_cycle_report(cycle, decision_analysis, cycle_time)
                
                cycle_time = time.time() - cycle_start_time
                self.logger.info(f"âœ… Ú†Ø±Ø®Ù‡ #{self.cycle_count} Ú©Ø§Ù…Ù„ Ø´Ø¯ Ø¯Ø± {cycle_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
                
                if cycle < max_cycles - 1:
                    sleep_time = 300
                    self.logger.info(f"â³ Ø§Ø³ØªØ±Ø§Ø­Øª Ø¨Ù‡ Ù…Ø¯Øª {sleep_time} Ø«Ø§Ù†ÛŒÙ‡")
                    time.sleep(sleep_time)
                
            except Exception as e:
                self.logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú†Ø±Ø®Ù‡ #{self.cycle_count}: {e}")
                time.sleep(30)
        
        self.enhanced_finalize_execution()
    
    def upload_enhanced_cycle_report(self, cycle: int, decision_analysis, cycle_time: float):
        """Ø¢Ù¾Ù„ÙˆØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú†Ø±Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        report = {
            'cycle_number': cycle,
            'timestamp': datetime.now().isoformat(),
            'cycle_duration_seconds': round(cycle_time, 2),
            'decision_analysis': decision_analysis,
            'knowledge_count': self.get_knowledge_stats(),
            'performance_metrics': self.evolution_system.evaluate_performance(),
            'system_health': self.get_system_health()
        }
        
        self.github.create_file_in_repo(
            f"cycles/enhanced_cycle_report_{cycle}.json",
            json.dumps(report, ensure_ascii=False, indent=2),
            f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú†Ø±Ø®Ù‡ Ø§Ø±ØªÙ‚Ø§ ÛŒØ§ÙØªÙ‡ #{cycle}"
        )
    
    def get_knowledge_stats(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¯Ø§Ù†Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        try:
            conn = sqlite3.connect(self.memory.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM conceptual_knowledge')
            total = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM conceptual_knowledge')
            categories = cursor.fetchone()[0]
            
            cursor.execute('SELECT AVG(confidence) FROM conceptual_knowledge')
            avg_confidence = cursor.fetchone()[0] or 0
            
            conn.close()
            
            return {
                'total_concepts': total,
                'category_diversity': categories,
                'average_confidence': round(avg_confidence, 3)
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¯Ø§Ù†Ø´: {e}")
            return {}
    
    def get_system_health(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
        try:
            return {
                'timestamp': datetime.now().isoformat(),
                'python_memory': psutil.Process().memory_info().rss / 1024 / 1024,
                'system_memory_usage': psutil.virtual_memory().percent,
                'cpu_usage': psutil.cpu_percent(interval=1),
                'disk_usage': psutil.disk_usage('.').percent
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…: {e}")
            return {}
    
    def enhanced_finalize_execution(self):
        """Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        self.logger.info("ğŸ Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ø®ÙˆØ¯Ù…Ø®ØªØ§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡")
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        final_evaluation = self.evolution_system.evaluate_performance()
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù¾ÛŒØ´Ø±ÙØª
        final_report = self.progress_dashboard.generate_daily_report()
        
        # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…
        system_state = {
            'final_cycle': self.cycle_count,
            'total_runtime': str(datetime.now() - self.start_time),
            'final_evaluation': final_evaluation,
            'progress_report': final_report,
            'github_operations': 'completed' if self.github_connected else 'failed',
            'next_scheduled_run': (datetime.now() + timedelta(hours=6)).isoformat()
        }
        
        if self.github_connected:
            self.github.create_file_in_repo(
                "system/enhanced_final_report.json",
                json.dumps(system_state, ensure_ascii=False, indent=2),
                "ğŸ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±ØªÙ‚Ø§ ÛŒØ§ÙØªÙ‡"
            )
        
        # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        final_summary = f"""
ğŸ¯ **Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø¬Ø±Ø§ÛŒ Sorna AI Nexus v{self.version}**

ğŸ“Š **Ø¢Ù…Ø§Ø± Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡:**
â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§: {self.cycle_count}
â€¢ Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§: {system_state['total_runtime']}
â€¢ Ø³Ø·Ø­ ØªÚ©Ø§Ù…Ù„: {final_evaluation.get('evolution_level', 1)}
â€¢ Ø§Ù…ØªÛŒØ§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯: {final_evaluation.get('performance_score', 0):.1%}

ğŸš€ **Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ÙØ¹Ø§Ù„:**
â€¢ Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±
â€¢ Ù…ÙˆØªÙˆØ± ØªØ­Ù‚ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯  
â€¢ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾ÛŒØ´Ø±ÙØª
â€¢ Ø±Ø¯ÛŒØ§Ø¨ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ

ğŸ’¡ **ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…:**
â€¢ Ø§ØªØµØ§Ù„ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: {'âœ… ÙØ¹Ø§Ù„' if self.github_connected else 'âŒ ØºÛŒØ±ÙØ¹Ø§Ù„'}
â€¢ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø±: âœ… ÙØ¹Ø§Ù„
â€¢ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´: âœ… ÙØ¹Ø§Ù„

ğŸ”„ **Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ: {system_state['next_scheduled_run']}**

âœ¨ **Sorna AI Nexus Ø¯Ø± Ø­Ø§Ù„ ØªÚ©Ø§Ù…Ù„...**
"""
        
        self.logger.evolution(final_summary)
        print(final_summary)

# ==================== Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
def main():
    print("ğŸ§  SORNA AI NEXUS - ENHANCED AUTONOMOUS SYSTEM")
    print("ğŸš€ Starting Enhanced Full Autonomy Mode...")
    print("ğŸ¯ New Features: Persistent Memory, Smart Research, Progress Dashboard")
    print("=" * 70)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
    os.makedirs("memory", exist_ok=True)
    os.makedirs("reports", exist_ok=True)
    os.makedirs("sorna_data", exist_ok=True)
    
    try:
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡
        sorna = SornaAutonomousAI()
        sorna.initialize_system()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±")
    except Exception as e:
        print(f"ğŸ’¥ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
